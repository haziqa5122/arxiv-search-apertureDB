{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUbaqq0guW8G",
    "outputId": "dda20051-cdba-4b98-a900-783fa4a332f5"
   },
   "outputs": [],
   "source": [
    "%pip install opendatasets openai unstructured[pdf] gradio langchain-openai aperturedb pandas langchain-community arxiv --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUjFDxxruW8H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import opendatasets as od\n",
    "from langchain_core.documents import Document\n",
    "from unstructured.partition.auto import partition\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain\n",
    ")\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.callbacks.manager import (\n",
    "    trace_as_chain_group,\n",
    ")\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHp9YYEHuW8I",
    "outputId": "40732856-558f-42bb-e435-d8c32d893425"
   },
   "outputs": [],
   "source": [
    "dataset = 'https://www.kaggle.com/datasets/Cornell-University/arxiv'\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul7tg_I6uW8I"
   },
   "outputs": [],
   "source": [
    "def fetch_paper_details(arxiv_id):\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[arxiv_id])))\n",
    "    paper.download_pdf( filename=f\"{arxiv_id}.pdf\")\n",
    "    return partition(f\"{arxiv_id}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_v-4B4auW8J",
    "outputId": "9a84d0dc-0ebb-4aa8-ea40-6a587205741c"
   },
   "outputs": [],
   "source": [
    "papers = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "sample = 2 # Arxiv has over 1.7M articles, using 20 for our application\n",
    "\n",
    "# Open the JSON file and process entries\n",
    "with open(\"arxiv/arxiv-metadata-oai-snapshot.json\", \"r\") as file:\n",
    "    for _ in range(sample):\n",
    "        line = file.readline()\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Extract relevant fields\n",
    "        arxiv_id = data.get(\"id\", \"\")\n",
    "\n",
    "        # Add paper details by downloading and parsing the paper\n",
    "        paper_details = \"\".join(\n",
    "            text if isinstance((text := element.text), str)\n",
    "            else \"\".join(str(part) for part in text) if isinstance(text, (list, tuple))\n",
    "            else str(text)\n",
    "            for element in fetch_paper_details(arxiv_id)\n",
    "        )\n",
    "        print(type(paper_details))\n",
    "        # Use LangChain's splitter to divide paper details into chunks\n",
    "        chunks = text_splitter.create_documents([paper_details])\n",
    "        print(len(chunks))\n",
    "        # Create a Document for each chunk\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(chunk,type(chunk))\n",
    "            document_id = f\"{arxiv_id}_{idx + 1}\"  # Unique ID for each chunk\n",
    "            document = Document(\n",
    "                page_content=chunk.page_content,\n",
    "                id=document_id,\n",
    "                metadata={\n",
    "                    'title': data.get(\"title\",\"\"),\n",
    "                    'authors': data.get(\"authors\", \"\"),\n",
    "                    'submitter': data.get(\"submitter\", \"\"),\n",
    "                    'abstract': data.get(\"abstract\", \"\"),\n",
    "                    'paper_content': chunk.page_content\n",
    "                }\n",
    "            )\n",
    "            papers.append(document)\n",
    "\n",
    "print(\"Processing complete. Papers saved to processed_papers.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AzeblmCfd9C",
    "outputId": "856bb557-cb78-4a37-fc21-f97e2bd8be02"
   },
   "outputs": [],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RxY5VISyQ1Y",
    "outputId": "7bcb1c3c-fd65-4b6c-a071-c50e10a72061"
   },
   "outputs": [],
   "source": [
    "!adb config create --active --from-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dso8GCzZuW8K",
    "outputId": "29a38fea-7f63-4938-f073-0253727e0639"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ApertureDB\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key  = \"<API-KEY>\")\n",
    "vector_db = ApertureDB.from_documents(papers, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-DO3xt-Op_R"
   },
   "outputs": [],
   "source": [
    "\n",
    "retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "llm = ChatOpenAI(\n",
    "    api_key  = \"<API-KEY>\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer user questions. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "--------------\n",
    "\n",
    "{context}\"\"\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(prompt_template)\n",
    "prompt = ChatPromptTemplate(\n",
    "\tmessages=[\n",
    "\t\tsystem_prompt,\n",
    "\t\tMessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "\t\tHumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "\t]\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "combine_docs_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    "    document_separator=\"---------\"\n",
    ")\n",
    "\n",
    "### Set up a chain that controls how the search query for the vectorstore is generated\n",
    "\n",
    "# This controls how the search query is generated.\n",
    "# Should take `chat_history` and `question` as input variables.\n",
    "template = \"\"\"Combine the chat history and follow up question into a a search query.\n",
    "\n",
    "Chat History:\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Follow up question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "### Create our function to use\n",
    "\n",
    "def qa_response(message, history):\n",
    "\n",
    "\t# Convert message history into format for the `question_generator_chain`.\n",
    "\tconvo_string = \"\\n\\n\".join([f\"Human: {h}\\nAssistant: {a}\" for h, a in history])\n",
    "\n",
    "\t# Convert message history into LangChain format for the final response chain.\n",
    "\tmessages = []\n",
    "\tfor human, ai in history:\n",
    "\t\tmessages.append(HumanMessage(content=human))\n",
    "\t\tmessages.append(AIMessage(content=ai))\n",
    "\n",
    "\t# Wrap all actual calls to chains in a trace group.\n",
    "\twith trace_as_chain_group(\"qa_response\") as group_manager:\n",
    "\n",
    "\t\t# Generate search query.\n",
    "\t\tsearch_query = question_generator_chain.run(\n",
    "\t\t\tquestion=message,\n",
    "\t\t\tchat_history=convo_string,\n",
    "\t\t\tcallbacks=group_manager\n",
    "\t\t)\n",
    "\n",
    "\t\t# Retrieve relevant docs.\n",
    "\t\tdocs = retriever.get_relevant_documents(search_query, callbacks=group_manager)\n",
    "\n",
    "\t\t# Answer question.\n",
    "\t\treturn combine_docs_chain.run(\n",
    "\t\t\tinput_documents=docs,\n",
    "\t\t\tchat_history=messages,\n",
    "\t\t\tquestion=message,\n",
    "\t\t\tcallbacks=group_manager\n",
    "\t\t)\n",
    "\n",
    "### Now we start the app!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "R9rZ-k-9UVHn",
    "outputId": "353e83cc-5c06-4e19-9ec5-171b8356bbd6"
   },
   "outputs": [],
   "source": [
    "gr.ChatInterface(qa_response).launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
