{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUbaqq0guW8G",
    "outputId": "e8723a1d-ebad-43d2-b0af-d49c2bfc9ee8"
   },
   "outputs": [],
   "source": [
    "%pip install opendatasets openai unstructured[pdf] gradio langchain-openai aperturedb pandas langchain-community arxiv --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUjFDxxruW8H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import opendatasets as od\n",
    "from langchain_core.documents import Document\n",
    "from unstructured.partition.auto import partition\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHp9YYEHuW8I",
    "outputId": "92f5131b-65c2-434d-d9d0-9dc650833911"
   },
   "outputs": [],
   "source": [
    "dataset = 'https://www.kaggle.com/datasets/Cornell-University/arxiv'\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul7tg_I6uW8I"
   },
   "outputs": [],
   "source": [
    "def fetch_paper_details(arxiv_id):\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[arxiv_id])))\n",
    "    paper.download_pdf( filename=f\"{arxiv_id}.pdf\")\n",
    "    return partition(f\"{arxiv_id}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_v-4B4auW8J",
    "outputId": "dbf2efae-e57b-46da-982d-3a3ce95d414e"
   },
   "outputs": [],
   "source": [
    "papers = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "sample = 20 # Arxiv has over 1.7M articles, using 20 for our application\n",
    "\n",
    "# Open the JSON file and process entries\n",
    "with open(\"arxiv/arxiv-metadata-oai-snapshot.json\", \"r\") as file:\n",
    "    for _ in range(sample):\n",
    "        line = file.readline()\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Extract relevant fields\n",
    "        arxiv_id = data.get(\"id\", \"\")\n",
    "\n",
    "        # Add paper details by downloading and parsing the paper\n",
    "        paper_details = \"\".join(\n",
    "            text if isinstance((text := element.text), str)\n",
    "            else \"\".join(str(part) for part in text) if isinstance(text, (list, tuple))\n",
    "            else str(text)\n",
    "            for element in fetch_paper_details(arxiv_id)\n",
    "        )\n",
    "        print(type(paper_details))\n",
    "        # Use LangChain's splitter to divide paper details into chunks\n",
    "        chunks = text_splitter.create_documents([paper_details])\n",
    "        print(len(chunks))\n",
    "        # Create a Document for each chunk\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(chunk,type(chunk))\n",
    "            document_id = f\"{arxiv_id}_{idx + 1}\"  # Unique ID for each chunk\n",
    "            document = Document(\n",
    "                page_content=chunk.page_content,\n",
    "                id=document_id,\n",
    "                metadata={\n",
    "                    'title': data.get(\"title\",\"\"),\n",
    "                    'authors': data.get(\"authors\", \"\"),\n",
    "                    'submitter': data.get(\"submitter\", \"\"),\n",
    "                    'abstract': data.get(\"abstract\", \"\"),\n",
    "                    'paper_content': chunk.page_content\n",
    "                }\n",
    "            )\n",
    "            papers.append(document)\n",
    "\n",
    "print(\"Processing complete. Papers saved to processed_papers.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AzeblmCfd9C",
    "outputId": "d53a3308-fad0-4a2e-dc72-9839f435375e"
   },
   "outputs": [],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RxY5VISyQ1Y",
    "outputId": "bf97bdce-f262-4b75-fd48-be0a372c811a"
   },
   "outputs": [],
   "source": [
    "!adb config create --active --from-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dso8GCzZuW8K",
    "outputId": "6a3a4c9d-7e49-48f1-9788-a46ca280ab6b"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ApertureDB\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key  = \"<API-KEY>\")\n",
    "vector_db = ApertureDB.from_documents(papers, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rOcf_0Oy_4C"
   },
   "outputs": [],
   "source": [
    "# Create prompt\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key  = \"<API-KEY>\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    ")\n",
    "# Create a chain that passes documents to an LLM\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "# Treat the vectorstore as a document retriever\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "# Create a RAG chain that connects the retriever to the LLM\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqJwCGSiX4G6"
   },
   "outputs": [],
   "source": [
    "def ask_question(question,history):\n",
    "    response = retrieval_chain.invoke({\"input\": question})\n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xeeWbBzYAyI",
    "outputId": "5ec88e8e-e175-4375-de1f-5ac58ba5088f"
   },
   "outputs": [],
   "source": [
    "ask_question(\"What is this paper about: Calulation of prompt diphoton prodution ross setions at Tevatron and LHC energies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "HpAivY2xBRkv",
    "outputId": "59d87525-8d51-4f2d-d7ca-f76772a004a5"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.ChatInterface(ask_question, type=\"messages\").launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
